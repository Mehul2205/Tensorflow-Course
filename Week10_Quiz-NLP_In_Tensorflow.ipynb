{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week10_Quiz.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPbwokTudt+E6bo7qUo05vF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8lrvjaadE_P",
        "colab_type": "text"
      },
      "source": [
        "# Quiz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu62P9jYdGfl",
        "colab_type": "text"
      },
      "source": [
        "Q1. What is the name of the TensorFlow library containing common data that you can use to train and test neural networks?\n",
        "\n",
        "Sol. TensorFlow Datasets\n",
        "\n",
        "Q2. How many reviews are there in the IMDB dataset and how are they split?\n",
        "\n",
        "Sol. 50,000 records, 50/50 train/test split\n",
        "\n",
        "Q3. How are the labels for the IMDB dataset encoded?\n",
        "\n",
        "Sol. Reviews encoded as a number 0-1\n",
        "\n",
        "Q4. What is the purpose of the embedding dimension?\n",
        "\n",
        "Sol. It is the number of dimensions for the vector representing the word encoding\n",
        "\n",
        "Q5. When tokenizing a corpus, what does the num_words=n parameter do?\n",
        "\n",
        "Sol. It specifies the maximum number of words to be tokenized, and picks the most common ‘n’ words\n",
        "\n",
        "Q6. To use word embeddings in TensorFlow, in a sequential layer, what is the name of the class?\n",
        "\n",
        "Sol. tf.keras.layers.Embedding\n",
        "\n",
        "Q7. IMDB Reviews are either positive or negative. What type of loss function should be used in this scenario?\n",
        "\n",
        "Sol. binary_crossentropy\n",
        "\n",
        "Q8. When using IMDB Sub Words dataset, our results in classification were poor. Why?\n",
        "\n",
        "Sol. Sequence becomes much more important when dealing with subwords, but we’re ignoring word positions"
      ]
    }
  ]
}