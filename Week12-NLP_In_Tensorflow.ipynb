{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week12-NLP_In_Tensorflow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN//Obt8s5gSzO7vwT2L6qy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mehul2205/Tensorflow-Course/blob/master/Week12-NLP_In_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7lJp9JWZSuX",
        "colab_type": "text"
      },
      "source": [
        "# Quiz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF3KlpbtZUS-",
        "colab_type": "text"
      },
      "source": [
        "Q1. What is the name of the method used to tokenize a list of sentences?\n",
        "\n",
        "Sol. fit_on_texts(sentences)\n",
        "\n",
        "Q2. If a sentence has 120 tokens in it, and a Conv1D with 128 filters with a Kernal size of 5 is passed over it, what’s the output shape?\n",
        "\n",
        "Sol. (None, 116, 128)\n",
        "\n",
        "Q3. What is the purpose of the embedding dimension?\n",
        "\n",
        "Sol. It is the number of dimensions for the vector representing the word encoding\n",
        "\n",
        "Q4. IMDB Reviews are either positive or negative. What type of loss function should be used in this scenario?\n",
        "\n",
        "Sol. Binary Crossentropy\n",
        "\n",
        "Q5. If you have a number of sequences of different lengths, how do you ensure that they are understood when fed into a neural network?\n",
        "\n",
        "Sol. Use the pad_sequences object from the tensorflow.keras.preprocessing.sequence namespace\n",
        "\n",
        "Q6. When predicting words to generate poetry, the more words predicted the more likely it will end up gibberish. Why?\n",
        "\n",
        "Sol. Because the probability that each word matches an existing phrase goes down the more words you create\n",
        "\n",
        "Q7. What is a major drawback of word-based training for text generation instead of character-based generation?\n",
        "\n",
        "Sol. Because there are far more words in a typical corpus than characters, it is much more memory intensive\n",
        "\n",
        "Q8. How does an LSTM help understand meaning when words that qualify each other aren’t necessarily beside each other in a sentence?\n",
        "\n",
        "Sol. Values from earlier words can be carried to later ones via a cell state"
      ]
    }
  ]
}